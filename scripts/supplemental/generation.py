import pandas as pd
import os, sys, time
import h5py
import logging
from settings import run_args
sys.path.append(os.path.join(os.getcwd(), "scripts"))
sys.path.append(os.path.join(os.getcwd(), "scripts/trucks"))
sys.path.append(os.getcwd())


def balance_trips(df, trip_purposes, balanced_to):
    """Balance trips to productions or attractions."""
    if balanced_to == "pro":
        to_balance = "att"

    else:
        to_balance = "pro"

    for purposes in trip_purposes:
        total_to_match = sum(df[purposes + balanced_to])
        total_to_balance = sum(df[purposes + to_balance])
        ratio = total_to_match / total_to_balance
        df[purposes + to_balance] = df[purposes + to_balance] * ratio

    return df


def create_df_from_h5(h5_file, h5_table, h5_variables):
    h5_data = {}
    for var in h5_variables:
        h5_data[var] = h5_file[h5_table][var][:]

    return pd.DataFrame(h5_data)


def calc_heavy_truck_restrictions(state):
    """Restrict truck trips by land use type."""

    #  Load land use type from parcels and a lookup for landuse type codes
    parcels = pd.read_csv(r"outputs/landuse/buffered_parcels.txt", sep="\s+")
    df = parcels.merge(
        pd.read_csv(
            f"inputs/model/{state.input_settings.abm_model}/lookup/lu_type.csv"
        ),
        left_on="lutype_p",
        right_on="land_use_type_id",
    )

    # The following list of land use types are allowed to be accessed by heavy trucks
    truck_uses = [
        "agriculture",
        "forest",
        "industrial",
        "military",
        "mining",
        "warehousing",
    ]

    truck_df = df[df["land_use_name"].isin(truck_uses)]
    truck_df["taz_p"].value_counts().index.values
    allowed_tazs = truck_df["taz_p"].value_counts().index.values

    return allowed_tazs


def main(state):
    print(
        "Calculating supplemental trips generated by exterals, special generators, and group quarters."
    )

    # Create a logging file to report model progress
    logging.basicConfig(
        filename=state.emme_settings.supplemental_log_file, level=logging.DEBUG
    )

    # Report model starting
    current_time = str(time.strftime("%H:%M:%S"))
    logging.debug("----Began generation.py script at " + current_time)

    # Trip production and attraction lists
    trip_purpose_list = ['hbw1', 'hbw2', 'hbw3', 'hbw4', 'col', 'hsp', 'hbo', 'sch', 'oto', 'wto']
    trip_productions = [trip_purpose + "pro" for trip_purpose in trip_purpose_list]
    trip_attractions = [trip_purpose + "att" for trip_purpose in trip_purpose_list]

    # List of columns that should be balanced to productions or attractions
    balance_to_productions = [
        "hbw1",
        "hbw2",
        "hbw3",
        "hbw4",
        "hsp",
        "hbo",
        "oto",
        "wto",
        "mtk",
        "htk",
        "cvh",
        "dtk",
    ]
    balance_to_attractions = ["col", "sch"]

    i5_station = state.emme_settings.EXTERNALS_DONT_GROW[0]

    # Lists for HH and Person Files
    hh_variables = ["hhno", "hhsize", "hhparcel", "hhincome"]
    person_variables = ["pno", "hhno", "pptyp"]

    employment_categories = [
        "retail",
        "food-services",
        "government",
        "office",
        "services",
        "industrial",
        "education",
        "medical",
        "other",
        "university",
        "total-hh",
        "total-jobs",
        "total-people",
    ]

    # Load parcel file with updated military parcels (processed in create_ixxi_work_trips.py previously)
    parcel_file = "outputs/landuse/parcels_urbansim.txt"

    output_directory = "outputs/supplemental"


    ###########################################################
    # PSRC Zone System for TAZ joining
    ###########################################################
    df_psrc = pd.read_sql("SELECT * FROM psrc_zones", con=state.conn)
    df_psrc["taz"] = df_psrc["taz"].astype(int)
    df_psrc = df_psrc.loc[:, ["taz", "county", "jblm", "external"]]

    ###########################################################
    # Auto External Stations
    ###########################################################

    df_external = pd.read_sql(
        "SELECT * FROM auto_externals where year="
        + str(state.input_settings.base_year),
        con=state.conn,
    )
    df_external["taz"] = df_external["taz"].astype(int)
    df_external = df_external.loc[
        :, ["taz", "year"] + trip_productions + trip_attractions
    ]
    data_year = int(df_external["year"][0])

    # Join to full TAZ file to ensure final merging works
    df_external = pd.merge(
        df_psrc, df_external, on="taz", suffixes=("_x", "_y"), how="left"
    )
    df_external.fillna(0, inplace=True)
    df_external.set_index("taz", inplace=True)
    df_external = df_external.loc[:, ["year"] + trip_productions + trip_attractions]
    df_external = df_external.astype(float)

    # Calculate the Inputs for the Year of the model
    if int(state.input_settings.model_year) > data_year:
        growth_rate = 1 + (
            state.emme_settings.external_rate
            * (int(state.input_settings.model_year) - data_year)
        )
        df_external = df_external * growth_rate

    external_taz = df_external.loc[:, trip_productions + trip_attractions]

    ###########################################################
    # Enlisted Personnel
    ###########################################################
    df_enlisted = pd.read_sql_query("SELECT * FROM enlisted_personnel", con=state.conn)
    df_enlisted["taz"] = df_enlisted["Zone"].copy()

    # Select data for model year only
    df_enlisted = df_enlisted[
        df_enlisted["year"] == int(state.input_settings.model_year)
    ][["taz", "military_jobs"]]

    # Aggregate enlisted personnel by TAZ
    enlisted_taz = df_enlisted.groupby("taz").sum().reset_index()
    enlisted_taz["taz"] = enlisted_taz["taz"].astype("int")

    # Read in rates and calculate Enlisted Personnel related trips by Purpose
    df_rates = pd.read_sql_query("SELECT * FROM trip_rates", con=state.conn)

    df_enlisted_rates = df_rates[df_rates["group"] == "enlisted"]

    for purpose in trip_attractions:
        enlisted_taz[purpose] = (
            enlisted_taz["military_jobs"] * df_enlisted_rates[purpose].values[0]
        )

    # Join to full TAZ file to ensure final merging works
    enlisted_taz = pd.merge(
        df_psrc, enlisted_taz, on="taz", suffixes=("_x", "_y"), how="left"
    )
    enlisted_taz.fillna(0, inplace=True)
    enlisted_taz.set_index("taz", inplace=True)
    enlisted_taz = enlisted_taz.loc[:, trip_attractions]

    ###########################################################
    # Group Quarters
    ###########################################################
    total_gq_df = pd.read_sql_query("SELECT * FROM group_quarters", con=state.conn)
    total_gq_df[['dorms', 'military', 'other']] = total_gq_df[['dorms', 'military', 'other']].astype("float")

    # Read in rates and calculate total Group Quarters related trips
    df_gq_rates = df_rates[df_rates["group"] == "group_quarters"]

    for purpose in trip_productions:
        for gq_type in ["dorms", "military", "other"]:
            total_gq_df[purpose] = (
                total_gq_df[gq_type]
                * df_gq_rates[df_gq_rates["segmentation"] == gq_type][purpose].values[0]
            )

    # Consolidate Group Quarters data to TAZ for output to travel model
    group_quarters_taz = total_gq_df.groupby("taz").sum()
    group_quarters_taz = group_quarters_taz.reset_index()
    group_quarters_taz["taz"] = group_quarters_taz["taz"].apply(int)

    # Join to full TAZ file to ensure final merging works
    group_quarters_taz = pd.merge(
        df_psrc, group_quarters_taz, on="taz", suffixes=("_x", "_y"), how="left"
    )
    group_quarters_taz.fillna(0, inplace=True)
    group_quarters_taz.set_index("taz", inplace=True)
    group_quarters_taz = group_quarters_taz[trip_productions]

    ###########################################################
    ### Joint Base Lewis McChord
    ###########################################################
    jblm_df = pd.read_sql_query("SELECT * FROM jblm_trips", con=state.conn)
    jblm_matrix = int(jblm_df["matrix_id"][0])
    data_year = int(jblm_df["year"][0])

    keep_columns = [
        "origin_zone",
        "destination_zone",
        "trips",
    ]
    jblm_df = jblm_df.loc[:, keep_columns]
    jblm_df["trips"] = jblm_df["trips"].apply(float)

    # JBLM trips are held constant, no need to scale for future years

    # Create JBLM Input File for use in Emme
    working_file = open(output_directory + "/jblm.in", "w")
    working_file.write(
        "c " + str(state.input_settings.model_year) + " Trip Generation" + "\n"
    )
    working_file.write(
        "c JBLM trips are based on gate counts, blue tooth and zipcode survey data"
        + "\n"
    )
    working_file.write("t matrices" + "\n")
    working_file.write(
        "a matrix=mf" + str(jblm_matrix) + " jblm " + "0 JBLM Trips" + "\n"
    )

    for rows in range(0, (len(jblm_df))):
        origin_zone = jblm_df["origin_zone"][rows]
        destination_zone = jblm_df["destination_zone"][rows]
        working_file.write(
            " "
            + str(origin_zone)
            + " "
            + str(destination_zone)
            + " : "
            + str(jblm_df["trips"][rows])
            + "\n"
        )

    working_file.close()

    ###########################################################
    # Remove JBLM Externals from the Externals dataframe
    ###########################################################

    # Calculate amount of JBLM traffic that is from/to an external and remove it from the External Station df
    jblm_df["origin_zone"] = jblm_df["origin_zone"].apply(int)
    jblm_df["destination_zone"] = jblm_df["destination_zone"].apply(int)

    jblm_external = jblm_df[
        (jblm_df["origin_zone"] >= state.emme_settings.MIN_EXTERNAL)
    ]
    jblm_ext_productions = sum(jblm_external["trips"])

    jblm_external = jblm_df[
        (jblm_df["destination_zone"] >= state.emme_settings.MIN_EXTERNAL)
    ]
    jblm_ext_attractions = sum(jblm_external["trips"])

    # Calculate the total trip productions and attractions for the I-5 Zone
    i5_ext_productions = 0
    for purposes in trip_productions:
        i5_ext_productions = i5_ext_productions + external_taz[purposes][i5_station]

    i5_ext_attractions = 0
    for purposes in trip_attractions:
        i5_ext_attractions = i5_ext_attractions + external_taz[purposes][i5_station]

    # Scale the I-5 station volumes by purpose
    i5_revised_productions = i5_ext_productions - jblm_ext_productions
    i5_revised_attractions = i5_ext_attractions - jblm_ext_attractions

    revised_external_taz = external_taz.copy()
    for purposes in trip_productions:
        ratio = revised_external_taz.loc[i5_station, purposes] / i5_ext_productions
        revised_external_taz.loc[i5_station, purposes] = i5_revised_productions * ratio

    for purposes in trip_attractions:
        ratio = revised_external_taz.loc[i5_station, purposes] / i5_ext_attractions
        revised_external_taz.loc[i5_station, purposes] = i5_revised_attractions * ratio

    ###########################################################
    # Heavy Truck Productions, grown from ATRI data
    ###########################################################
    heavy_trucks = pd.read_sql_query("SELECT * FROM heavy_trucks", con=state.conn)
    heavy_trucks = heavy_trucks[["taz", "year", "htkpro", "htkatt"]]

    # Calculate the Inputs for the Year of the model
    data_year = int(heavy_trucks["year"][0])
    heavy_trucks = heavy_trucks.drop("year", axis=1)
    heavy_trucks["htkpro"] = heavy_trucks["htkpro"].apply(float)
    heavy_trucks["htkatt"] = heavy_trucks["htkatt"].apply(float)

    if int(state.input_settings.model_year) > data_year:
        growth_rate = 1 + (
            state.emme_settings.truck_rate
            * (int(state.input_settings.model_year) - data_year)
        )
        heavy_trucks["htkpro"] = heavy_trucks["htkpro"] * growth_rate
        heavy_trucks["htkatt"] = heavy_trucks["htkatt"] * growth_rate

    # Make the TAZ field the index
    heavy_trucks_taz = heavy_trucks.groupby("taz").sum()
    heavy_trucks_taz = heavy_trucks_taz.reset_index()
    heavy_trucks_taz["taz"] = heavy_trucks_taz["taz"].apply(int)

    # Apply land-use restrictions for productions/attraction in TAZs without appropriate industrial uses
    allowed_tazs = calc_heavy_truck_restrictions(state)
    external_taz_list = list(
        range(state.emme_settings.MIN_EXTERNAL, state.emme_settings.MAX_EXTERNAL + 1)
    )
    allowed_tazs = allowed_tazs.tolist() + external_taz_list

    heavy_trucks_taz = heavy_trucks_taz[heavy_trucks_taz["taz"].isin(allowed_tazs)]

    # Join to full TAZ file to ensure final merging works
    heavy_trucks_taz = pd.merge(
        df_psrc, heavy_trucks_taz, on="taz", suffixes=("_x", "_y"), how="left"
    )
    heavy_trucks_taz.fillna(0, inplace=True)
    heavy_trucks_taz = heavy_trucks_taz.loc[:, ["taz", "htkpro", "htkatt"]]

    ###########################################################
    # SeaTac Airport
    ###########################################################
    df_seatac = pd.read_sql_query("SELECT * FROM seatac", con=state.conn)
    seatac_zone = df_seatac[df_seatac["year"] == int(state.input_settings.model_year)][
        "taz"
    ].values[0]
    seatac_enplanements = df_seatac[
        df_seatac["year"] == int(state.input_settings.model_year)
    ]["enplanements"].values[0]

    ###########################################################
    ## Special Generators
    ###########################################################

    # Special generator trips are assumed of type HBO
    df_special = pd.read_sql_query("SELECT * FROM special_generators", con=state.conn)

    # Calculate the Inputs for the Year of the model
    max_input_year = df_special["year"].max()

    if int(state.input_settings.model_year) <= max_input_year:
        df_special = df_special[df_special["year"] == state.input_settings.model_year]
        df_special["hboatt"] = df_special["trips"].astype("float")

    else:
        df_special = df_special[df_special["year"] == max_input_year]
        df_special["hboatt"] = df_special[df_special["year"] == max_input_year][
            "trips"
        ].astype("float")

        df_special["hboatt"] = df_special["hboatt"] * (
            1
            + (
                state.emme_settings.special_generator_rate
                * (int(state.input_settings.model_year) - max_input_year)
            )
        )

    df_special = df_special[["taz", "hboatt"]]

    # Consolidate Special Generator data to TAZ
    special_generators_taz = df_special.groupby("taz").sum()
    special_generators_taz = special_generators_taz.reset_index()
    special_generators_taz["taz"] = special_generators_taz["taz"].apply(int)

    # Join to full TAZ file to ensure final merging works
    special_generators_taz = pd.merge(
        df_psrc, special_generators_taz, on="taz", suffixes=("_x", "_y"), how="left"
    )
    special_generators_taz.fillna(0, inplace=True)
    special_generators_taz.set_index("taz", inplace=True)
    special_generators_taz = special_generators_taz.loc[:, ["hboatt"]]

    ###########################################################
    # Load Household, Person and Parcel files
    ###########################################################

    # Parcel Columns to use and what to rename them
    parcel_col_map = {
        "parcelid": "parcel-id",
        "xcoord_p": "xcoord",
        "ycoord_p": "ycoord",
        "taz_p": "taz",
        "empedu_p": "education",
        "empfoo_p": "food-services",
        "empgov_p": "government",
        "empind_p": "industrial",
        "empmed_p": "medical",
        "empofc_p": "office",
        "empret_p": "retail",
        "emprsc_p": "resources",
        "empsvc_p": "services",
        "empoth_p": "other",
        "emptot_p": "total-jobs",
        "stugrd_p": "k-8",
        "stuhgh_p": "high-school",
        "stuuni_p": "university",
    }
    if state.input_settings.abm_model == "activitysim":
        hh_df = pd.read_csv(os.path.join(run_args.args.data_dir, "households.csv"))
        person_df = pd.read_csv(os.path.join(run_args.args.data_dir, "persons.csv"))
        # FIXME: make sure ptype has the same coding as pptyp in daysim
        hh_df = hh_df.drop('workers', axis=1)

        person_type = "ptype"
        household_id = "household_id"
        person_number = "PNUM"
        hhsize = "PERSONS"
        income = "income"
    else:
        hh_person = r"inputs/scenario/landuse/hh_and_persons.h5"
        hh_people = h5py.File(hh_person, "r+")
        hh_df = create_df_from_h5(hh_people, "Household", hh_variables)
        person_df = create_df_from_h5(hh_people, "Person", person_variables)
        person_type = "pptyp"
        household_id = "hhno"
        person_number = "pno"
        hhsize = "hhsize"
        income = "hhincome"

    # Parcel inputs are use for both Daysim and Activitysim
    parcels = pd.read_csv(parcel_file, sep=" ")
    parcels = parcels.rename(columns=parcel_col_map)

    ###########################################################
    # Create a Household file with cross classifications
    # using the person and household file
    ###########################################################

    person_df["people"] = 1

    # Flag if the person has a full or part-time job
    person_df["workers"] = 0
    person_df.loc[person_df[person_type] == 1, "workers"] = 1
    person_df.loc[person_df[person_type] == 2, "workers"] = 1

    # Flag if the person is a school age kid
    person_df["school-age"] = 0
    person_df.loc[person_df[person_type] == 6, "school-age"] = 1
    person_df.loc[person_df[person_type] == 7, "school-age"] = 1

    # Flag if the person is a college student
    person_df["college-student"] = 0
    person_df.loc[person_df[person_type] == 5, "college-student"] = 1

    # Remove a couple columns
    fields_to_remove = [person_number, person_type]
    person_df = person_df.drop(fields_to_remove, axis=1)

    # Create a HH file by grouping the person file by household number
    df_hh = person_df.groupby(household_id).sum()
    df_hh = df_hh.reset_index()

    # Merge the HH File created from the persons with the original HH file from H5
    df_hh = pd.merge(df_hh, hh_df, left_on=household_id, right_on="hhno", suffixes=("_x", "_y"), how="left")
    # Create a Column for Household sizes 1, 2, 3 or 4+
    df_hh["household-class"] = df_hh[hhsize]
    df_hh.loc[df_hh["household-class"] > 4, "household-class"] = 4

    # Create a Column for workers 0, 1, 2 or 3+
    df_hh["worker-class"] = df_hh["workers"]
    df_hh.loc[df_hh["worker-class"] > 3, "worker-class"] = 3

    # Create a Column for Income 1, 2 ,3 or 4
    df_hh["income-class"] = 0
    df_hh.loc[df_hh[income] <= state.emme_settings.low_income, "income-class"] = 1
    df_hh.loc[
        (df_hh[income] > state.emme_settings.low_income)
        & (df_hh[income] <= state.emme_settings.medium_income),
        "income-class",
    ] = 2
    df_hh.loc[
        (df_hh[income] > state.emme_settings.medium_income)
        & (df_hh[income] <= state.emme_settings.high_income),
        "income-class",
    ] = 3
    df_hh.loc[df_hh[income] > state.emme_settings.high_income, "income-class"] = 4

    # Create a Column for school age children 0, 1, 2 or 3+
    df_hh["school-class"] = df_hh["school-age"]
    df_hh.loc[df_hh["school-class"] > 3, "school-class"] = 3

    # Create a Column for college age persons 0, 1, 2+
    df_hh["college-class"] = df_hh["college-student"]
    df_hh.loc[df_hh["college-class"] > 2, "college-class"] = 2

    # Create a Columns for Household - Work - Income Cross-Classification, Income & School and Income and College
    df_hh["hwi"] = (
        "h"
        + df_hh["household-class"].apply(str)
        + "w"
        + df_hh["worker-class"].apply(str)
        + "i"
        + df_hh["income-class"].apply(str)
    )
    df_hh["si"] = (
        "s" + df_hh["school-class"].apply(str) + "i" + df_hh["income-class"].apply(str)
    )
    df_hh["ci"] = (
        "c" + df_hh["college-class"].apply(str) + "i" + df_hh["income-class"].apply(str)
    )

    ###########################################################
    # Household trip production
    ###########################################################
    df_hh_rates = df_rates[df_rates["group"] == "household"].drop(
        ["schpro", "schatt", "colpro", "colatt"], axis=1
    )
    df_sch_rates = df_rates[df_rates["group"] == "school"][
        ["segmentation", "schpro", "schatt"]
    ]
    df_coll_rates = df_rates[df_rates["group"] == "college"][
        ["segmentation", "colpro", "colatt"]
    ]

    # Merge Rates with Households by Cross-Classification so we end up with total household productions
    df_hh = pd.merge(
        df_hh,
        df_hh_rates,
        left_on="hwi",
        right_on="segmentation",
        suffixes=("_x", "_y"),
        how="left",
    )
    df_hh = pd.merge(
        df_hh,
        df_sch_rates,
        left_on="si",
        right_on="segmentation",
        suffixes=("_x", "_y"),
        how="left",
    )
    df_hh = pd.merge(
        df_hh,
        df_coll_rates,
        left_on="si",
        right_on="segmentation",
        suffixes=("_x", "_y"),
        how="left",
    )
    df_hh = df_hh.loc[:, trip_productions + trip_attractions + ["hhparcel", "people"]]
    df_hh["total-hh"] = 1

    ###########################################################
    # Combine HH Trip Generation with Parcels
    ###########################################################
    df_parcel_hh_pa = df_hh.groupby("hhparcel").sum()
    df_parcel_hh_pa = df_parcel_hh_pa.reset_index()
    df_parcel_hh_pa.rename(
        columns={"hhparcel": "parcel-id", "people": "total-people"}, inplace=True
    )

    ###########################################################
    # Parcel trip attractions
    ###########################################################
    df_parcels = pd.merge(
        parcels, df_parcel_hh_pa, on="parcel-id", suffixes=("_x", "_y"), how="left"
    )
    df_parcels.fillna(0, inplace=True)

    # Create a couple columns for trip attractions for parcels
    df_parcels["education"] = df_parcels["k-8"] + df_parcels["high-school"]
    df_parcels["mtkpro"] = 0
    df_parcels["mtkatt"] = 0
    df_parcels["cvhpro"] = 0
    df_parcels["cvhatt"] = 0
    df_parcels["dtkatt"] = 0
    df_parcels["dtkpro"] = 0

    # Trip Attractions based on employment categories
    df_job_attraction_rates = pd.read_sql_query(
        "SELECT * FROM job_attractions", con=state.conn
    )
    df_job_attraction_rates.set_index("employment-type", inplace=True)
    attraction_purposes = trip_attractions + ["cvhatt", "mtkatt", "dtkatt"]

    for purpose in attraction_purposes:
        for jobs in employment_categories:
            df_parcels[purpose] = df_parcels[purpose] + (
                df_parcels[jobs] * df_job_attraction_rates.loc[jobs, purpose]
            )

    # Trip Productions based on employment categories
    df_job_production_rates = pd.read_sql_query(
        "SELECT * FROM job_productions", con=state.conn
    )
    df_job_production_rates.set_index("employment-type", inplace=True)
    productions_purposes = trip_productions + ["cvhpro", "mtkpro", "dtkpro"]

    for purpose in productions_purposes:
        for jobs in employment_categories:
            df_parcels[purpose] = df_parcels[purpose] + (
                df_parcels[jobs] * df_job_production_rates.loc[jobs, purpose]
            )

    # Scale delivery productions based on a target number of delivery trips
    df_parcels["dtkpro"] = state.emme_settings.total_delivery_trips * (
        df_parcels["dtkpro"] / df_parcels["dtkpro"].sum()
    )

    ###########################################################
    # SeaTac Airport trip generation
    ###########################################################
    df_parcels["airport"] = (
        df_parcels["total-jobs"] * state.emme_settings.air_jobs
    ) + (df_parcels["total-people"] * state.emme_settings.air_people)
    aiport_balancing = seatac_enplanements / sum(df_parcels["airport"])
    df_parcels["airport"] = df_parcels["airport"] * aiport_balancing

    ###########################################################
    # Create TAZ Input files
    ###########################################################
    df_taz = df_parcels.groupby("taz").sum()
    df_taz = df_taz.reset_index()
    df_taz.fillna(0, inplace=True)
    df_taz["taz"] = df_taz["taz"].apply(int)

    # Join to full TAZ file to ensure final merging works
    df_taz = pd.merge(df_psrc, df_taz, on="taz", suffixes=("_x", "_y"), how="left")
    df_taz.fillna(0, inplace=True)

    # Create a Kitsap County flag for use in Kitsap Adjustments
    df_taz["kitsap"] = 0
    df_taz.loc[df_taz["county"] == "Kitsap", "kitsap"] = 1

    # Add in Heavy Truck Productions and Attractions
    df_taz = pd.merge(
        df_taz, heavy_trucks_taz, on="taz", suffixes=("_x", "_y"), how="left"
    )
    df_taz.fillna(0, inplace=True)

    # Clean up dataframe for further calculations as well as output
    df_taz.set_index("taz", inplace=True)
    df_taz = df_taz.loc[
        :,
        trip_productions
        + ["cvhpro", "mtkpro", "htkpro", "dtkpro"]
        + trip_attractions
        + ["cvhatt", "mtkatt", "htkatt", "dtkatt", "airport", "kitsap", "jblm"],
    ]
    df_taz.to_csv(output_directory + "/1_unadjusted_unbalanced.csv", index=True)

    # Add in the Group Quarters to Trip Productions
    for purpose in trip_productions:
        df_taz[purpose] = df_taz[purpose] + group_quarters_taz[purpose]
    df_taz.to_csv(output_directory + "/2_add_group_quarters.csv", index=True)

    # Add in the Enlisted Personnel to Trip Attractions
    for purpose in trip_attractions:
        df_taz[purpose] = df_taz[purpose] + enlisted_taz[purpose]
    df_taz.to_csv(output_directory + "/3_add_enlisted_personnel.csv", index=True)

    # Add in the Special Generators to Trip Attractions
    df_taz["hboatt"] = df_taz["hboatt"] + special_generators_taz["hboatt"]
    df_taz.to_csv(output_directory + "/4_add_special_generators.csv", index=True)

    # Add in the External Trips
    all_purposes = trip_productions + trip_attractions
    for purpose in all_purposes:
        df_taz[purpose] = df_taz[purpose] + revised_external_taz[purpose]
    df_taz.to_csv(output_directory + "/5_add_externals.csv", index=True)

    # Soundcast uses pre-determined HSP trips to meet external counts. Need to adjust these here for non-work-ixxi:
    external_trip_table = pd.read_sql(
        "SELECT * FROM externals_unadjusted where year="
        + str(state.input_settings.base_year),
        con=state.conn,
    )
    external_trip_table.set_index("taz", inplace=True)
    external_trip_table = external_trip_table[["hsppro", "hspatt"]]
    df_taz.update(external_trip_table)

    # Zero out JBLM trips that were generated above (so only inlcude Shopping, HBO, OtO and WtO)
    df_taz["jblm"] = df_taz["jblm"].apply(int)
    jblm_purposes = [
        "hbw1pro",
        "hbw2pro",
        "hbw3pro",
        "hbw4pro",
        "colpro",
        "schpro",
        "cvhpro",
        "mtkpro",
        "htkpro",
        "hbw1att",
        "hbw2att",
        "hbw3att",
        "hbw4att",
        "colatt",
        "schatt",
        "cvhatt",
        "mtkatt",
        "htkatt",
    ]

    for purposes in jblm_purposes:
        df_taz.loc[df_taz["jblm"] == 1, purposes] = 0

    # Adjust the taz level data based on trip rate adjustments
    df_rate_adjustments = pd.read_sql_query("SELECT * FROM rate_adjustments", con=state.conn)
    df_rate_adjustments.set_index("trip-purpose", inplace=True)
    all_purposes = (
        trip_productions
        + ["cvhpro", "mtkpro", "htkpro"]
        + trip_attractions
        + ["cvhatt", "mtkatt", "htkatt"]
    )

    # Adjust Productions and Attractions by Adjustment Factors
    for purpose in all_purposes:
        df_taz[purpose] = df_taz[purpose] * [
            df_rate_adjustments.loc[purpose, "regional"]
        ]
        df_taz[purpose] = df_taz[purpose] + (
            df_taz[purpose]
            * [df_rate_adjustments.loc[purpose, "kitsap"]]
            * df_taz["kitsap"]
        )

    df_taz.to_csv(output_directory + "/6_adjust_trip_ends.csv", index=True)

    # Balance the taz dataframe
    balanced_df = balance_trips(df_taz, balance_to_productions, "pro")
    balanced_df = balance_trips(df_taz, balance_to_attractions, "att")
    balanced_df.to_csv(output_directory + "/7_balance_trip_ends.csv", index=True)

    # my_project.close()


if __name__ == "__main__":
    main()
